{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU6u2D5Yw/aNVx5kPIJqKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorDiamandi/base_ML_Project/blob/BestResult/Colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMjKdkHdlpDo",
        "outputId": "90e192c7-b966-4700-c68f-93dbab1da8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'train.csv' already exists. Skipping download.\n",
            "File 'valid.csv' already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pathlib import Path\n",
        "\n",
        "def download_from_gdrive(url, filename):\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = url.split('/')[-2]\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download the file\n",
        "    if Path(filename).exists():\n",
        "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
        "    else:\n",
        "        gdown.download(download_url, filename, quiet=False)\n",
        "        print(f\"File downloaded as: {filename}\")\n",
        "\n",
        "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
        "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
        "# Example usage\n",
        "\n",
        "download_from_gdrive(train, 'train.csv')\n",
        "download_from_gdrive(valid, 'valid.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_depth, level_of_parallelism, number_of_trees,\n",
        "                             max_features, min_samples_split, min_samples_leaf):\n",
        "    for depth in tree_depth:\n",
        "        model = RandomForestRegressor(\n",
        "                random_state=100,\n",
        "                n_jobs=level_of_parallelism,\n",
        "                n_estimators=number_of_trees,\n",
        "                max_depth=depth,\n",
        "                max_features=max_features,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_samples_leaf=min_samples_leaf)\n",
        "\n",
        "\n",
        "        print('Fitting the model...')\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        print('Testing the model...')\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        rmse_test = get_rmse(y_test, y_test_pred)\n",
        "        rmse_train = get_rmse(y_train, y_train_pred)\n",
        "\n",
        "        print(f'Tree depth - {depth}')\n",
        "        print(f'STD Test - {y_test.std()}')\n",
        "        print(f'STD Train - {y_train.std()}')\n",
        "        print(f'RMSE Test - {rmse_test}')\n",
        "        print(f'RMSE Train - {rmse_train}')\n",
        "        print(f'Test/Train Ratio - {1-rmse_train/rmse_test}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kOhM8hbAZ2Ko"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.preprocessing"
      ],
      "metadata": {
        "id": "YMaFKj0o9URd",
        "outputId": "8382a995-0c4a-4ca8-e374-cd66b074e30a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn.preprocessing\n",
            "  Downloading sklearn_preprocessing-0.1.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: sklearn.preprocessing\n",
            "Successfully installed sklearn.preprocessing-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "#import category_encoders as ce\n",
        "\n",
        "# Function to retrieve statistic parameters from numerical columns of the train dataframe\n",
        "def compute_statistics(df):\n",
        "    numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "    mean_values = numeric_df.mean()\n",
        "    iqr_values = numeric_df.quantile(0.75) - numeric_df.quantile(0.25)\n",
        "    zscore_values = (numeric_df.mean() / numeric_df.std()).mean()\n",
        "\n",
        "    # Mean without extremes (assuming extremes are values outside 1.5*IQR)\n",
        "    def mean_without_extremes(series):\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        filtered_series = series[(series >= Q1 - 1.5 * IQR) & (series <= Q3 + 1.5 * IQR)]\n",
        "        return filtered_series.mean()\n",
        "\n",
        "    mean_no_extremes_values = numeric_df.apply(mean_without_extremes)\n",
        "\n",
        "    statistics_df = pd.DataFrame({\n",
        "        'mean': mean_values,\n",
        "        'iqr': iqr_values,\n",
        "        'zscore': [zscore_values] * len(numeric_df.columns),\n",
        "        'mean_without_extremes': mean_no_extremes_values\n",
        "    })\n",
        "\n",
        "    statistics_df = statistics_df.T\n",
        "    return statistics_df\n",
        "\n",
        "\n",
        "def get_stat_value(method, column, statistics_df):\n",
        "    if method not in statistics_df.index:\n",
        "        raise ValueError(f\"Method {method} not found in statistics DataFrame\")\n",
        "    return statistics_df.loc[method, column]\n",
        "\n",
        "\n",
        "def replace_outliers(df, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "\n",
        "    # Define the threshold for identifying outliers\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the statistical method value\n",
        "    df[column] = df[column].apply(lambda x: stat_value if x < lower_bound or x > upper_bound else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def replace_nans(df, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "    df[column] = df[column].fillna(stat_value)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_rmse(y, y_pred):\n",
        "    return mean_squared_error(y, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def split_product_class_series(series):\n",
        "    equipment_type = []\n",
        "    details = []\n",
        "\n",
        "    for item in series:\n",
        "        if pd.isna(item):\n",
        "            equipment_type.append(None)\n",
        "            details.append(None)\n",
        "        else:\n",
        "            split_item = item.split(' - ', 1)\n",
        "            equipment_type.append(split_item[0])\n",
        "            details.append(split_item[1] if len(split_item) > 1 else None)\n",
        "\n",
        "    return pd.Series(equipment_type), pd.Series(details)\n",
        "\n",
        "\n",
        "def replace_nan_with_string(df):\n",
        "    le = LabelEncoder()\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            if df[col].apply(type).nunique() > 1:\n",
        "                df[col] = df[col].fillna('Missing').astype(str)\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "\n",
        "\n",
        "# function to apply target encoding to selected columns\n",
        "def apply_target_encoding(df_train, df_test, df_valid, target_column, columns_to_encode):\n",
        "    X_train = df_train.drop(target_column, axis=1)\n",
        "    y_train = df_train[target_column]\n",
        "\n",
        "    target_encoder = TargetEncoder(columns_to_encode)\n",
        "    X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
        "    X_test_encoded = target_encoder.transform(df_test.drop(target_column, axis=1))\n",
        "    X_valid_encoded = target_encoder.transform(df_valid.drop(target_column, axis=1))\n",
        "\n",
        "    X_train_encoded = pd.concat([X_train_encoded, y_train.reset_index(drop=True)], axis=1)\n",
        "    X_test_encoded = pd.concat([X_test_encoded, df_test[target_column].reset_index(drop=True)], axis=1)\n",
        "    X_valid_encoded = pd.concat([X_valid_encoded, df_valid[target_column].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    return X_train_encoded, X_test_encoded, X_valid_encoded;\n",
        "\n",
        "\n",
        "def apply_one_hot_encoder(df_train, df_test, df_valid, columns_to_encode):\n",
        "    one_hot_encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('onehot', one_hot_encoder, columns_to_encode),\n",
        "    ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    X_train_encoded = preprocessor.fit_transform(df_train)\n",
        "    X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=preprocessor.get_feature_names_out())\n",
        "    X_Test_encoded = preprocessor.transform(df_test)\n",
        "    X_Test_encoded_df = pd.DataFrame(X_Test_encoded, columns=preprocessor.get_feature_names_out())\n",
        "    X_valid_encoded = preprocessor.transform(df_valid)\n",
        "    X_valid_encoded_df = pd.DataFrame(X_valid_encoded, columns=preprocessor.get_feature_names_out())\n",
        "\n",
        "    return X_train_encoded_df, X_Test_encoded_df, X_valid_encoded_df;\n",
        "\n",
        "\n",
        "def unite_sparse_columns(df, columns_to_unite, new_column_name):\n",
        "    columns_to_unite = [col for col in columns_to_unite if col != new_column_name]\n",
        "\n",
        "    existing_columns = [col for col in columns_to_unite if col in df.columns]\n",
        "    missing_columns = [col for col in columns_to_unite if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: The following columns are missing and will be ignored: {missing_columns}\")\n",
        "\n",
        "    if not existing_columns:\n",
        "        raise ValueError(\"None of the specified columns to unite exist in the DataFrame.\")\n",
        "\n",
        "    df[new_column_name + '_non_null_count'] = df[existing_columns].notnull().sum(axis=1)\n",
        "\n",
        "    df[new_column_name + '_any_non_null'] = df[existing_columns].notnull().any(axis=1).astype(int)\n",
        "\n",
        "    if df[existing_columns].apply(lambda col: col.map(lambda x: isinstance(x, (int, float)))).all().all():\n",
        "        df[new_column_name + '_sum'] = df[existing_columns].sum(axis=1, skipna=True)\n",
        "\n",
        "    if df[existing_columns].apply(lambda col: col.map(lambda x: isinstance(x, str))).all().all():\n",
        "        df[new_column_name + '_mode'] = df[existing_columns].mode(axis=1)[0]\n",
        "\n",
        "    df = df.drop(columns=existing_columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def missing_values_imputation(df, target_column, feature_columns):\n",
        "\n",
        "    df_notnull = df.dropna(subset=[target_column])\n",
        "    df_null = df[df[target_column].isnull()]\n",
        "\n",
        "    if df_null.empty:\n",
        "        return df\n",
        "\n",
        "    X = df_notnull[feature_columns]\n",
        "    y = df_notnull[target_column]\n",
        "    X_null = df_null[feature_columns]\n",
        "\n",
        "    categorical_features = [col for col in feature_columns if df[col].dtype == 'object']\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', DecisionTreeClassifier(random_state=0))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "\n",
        "    df.loc[df[target_column].isnull(), target_column] = model.predict(X_null)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_fiProductClassDesc(df, column_name):\n",
        "    # Function to parse each value in the fiProductClassDesc column\n",
        "    def parse_fiProductClassDesc(value):\n",
        "        match = re.match(r'^(.*) - (\\d+\\.\\d+) to (\\d+\\.\\d+) (.*)$', value)\n",
        "        if match:\n",
        "            category, low_value, high_value, characteristic = match.groups()\n",
        "            return category, float(low_value), float(high_value), characteristic\n",
        "        else:\n",
        "            return None, None, None, None\n",
        "\n",
        "    parsed_values = df[column_name].apply(parse_fiProductClassDesc)\n",
        "\n",
        "    df['Category'] = parsed_values.apply(lambda x: x[0])\n",
        "    df['LowValue'] = parsed_values.apply(lambda x: x[1])\n",
        "    df['HighValue'] = parsed_values.apply(lambda x: x[2])\n",
        "    df['Characteristic'] = parsed_values.apply(lambda x: x[3])\n",
        "\n",
        "    unique_characteristics = df['Characteristic'].dropna().unique()\n",
        "    for characteristic in unique_characteristics:\n",
        "        df[f'post_fi_{characteristic}'] = df.apply(\n",
        "            lambda row: f\"{row['LowValue']} to {row['HighValue']}\" if row['Characteristic'] == characteristic else None,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    df = df.drop(columns=['LowValue', 'HighValue', 'Characteristic'])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ju1gMoUpbq2b"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTYPE_SPEC = {\n",
        "    13: 'str',\n",
        "    39: 'str',\n",
        "    40: 'str',\n",
        "    41: 'str'}\n",
        "\n",
        "df = pd.read_csv('train.csv', dtype=DTYPE_SPEC)\n",
        "df_valid = pd.read_csv('valid.csv', dtype=DTYPE_SPEC)"
      ],
      "metadata": {
        "id": "KE2pPmGchFHY"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Constants\n",
        "LEVEL_OF_PARALLELISM = -1\n",
        "NUMBER_OF_TREES = 100\n",
        "TREE_DEPTH = 8\n",
        "MIN_SAMPLES_SPLIT = 3\n",
        "MIN_SAMPLES_LEAF = 2\n",
        "MAX_FEATURES = 0.9"
      ],
      "metadata": {
        "id": "IUvB7U6x93nX"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KILL_THEM_COLUMNS = ['uControl_any_non_null', 'uMounting_any_non_null', 'uHydraulics_non_null_count',\n",
        "                     'uControl_non_null_count', 'fiProductClassDesc']\n",
        "\n",
        "SIZE_FIT_COLUMNS = ['fiModelDesc', 'post_fi_Horsepower', 'Drive_System', 'Stick_Length', 'Undercarriage_Pad_Width',\n",
        "                    'Pad_Type', 'Differential_Type']\n",
        "\n",
        "DUPLICATE_COLUMNS = ['fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor']\n",
        "COLUMN_GROUPS = {\n",
        "    'uBladeStick': ['Blade_Extension', 'Stick_Length', 'Stick',], #'Blade_Type' 'Blade_Width'\n",
        "    'uTrack': ['Pad_Type', 'Grouser_Type', 'Grouser_Tracks'], #'Track_Type' 'Undercarriage_Pad_Width'\n",
        "    'uMounting': ['Backhoe_Mounting', 'Forks', 'Pushblock', 'Ripper', 'Scarifier', 'Thumb'],\n",
        "    'uControl': ['Travel_Controls', 'Ride_Control', 'Transmission',\n",
        "                 'Pattern_Changer', 'Tip_Control', 'Coupler', 'Coupler_System'], #'Steering_Controls'\n",
        "    'uHydraulics': ['Turbocharged'], #'Hydraulics' 'Hydraulics_Flow'\n",
        "    'uDrive': ['Differential_Type', 'Drive_System']\n",
        "    }"
      ],
      "metadata": {
        "id": "k_QiWkna-AuA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = split_fiProductClassDesc(df, 'fiProductClassDesc')\n",
        "df_valid = split_fiProductClassDesc(df_valid, 'fiProductClassDesc')\n",
        "\n",
        "df = missing_values_imputation(df, 'ProductSize', SIZE_FIT_COLUMNS)\n",
        "df_valid = missing_values_imputation(df_valid, 'ProductSize', SIZE_FIT_COLUMNS)"
      ],
      "metadata": {
        "id": "QCXS6vKJ-yHG"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for new_column, columns in COLUMN_GROUPS.items():\n",
        "    df = unite_sparse_columns(df, columns, new_column)\n",
        "    df_valid = unite_sparse_columns(df_valid, columns, new_column)"
      ],
      "metadata": {
        "id": "yqIeRds9-5KZ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(KILL_THEM_COLUMNS, axis=1, inplace=True)\n",
        "df_valid.drop(KILL_THEM_COLUMNS, axis=1, inplace=True)\n",
        "\n",
        "df.drop(DUPLICATE_COLUMNS, axis=1, inplace=True)\n",
        "df_valid.drop(DUPLICATE_COLUMNS, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "Z_IUmRIx-8l1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['saleyear'] = pd.to_datetime(df['saledate']).dt.year\n",
        "df_valid['saleyear'] = pd.to_datetime(df_valid['saledate']).dt.year\n",
        "df['age'] = 2024 - df['YearMade']\n",
        "df_valid['age'] = 2024 - df_valid['YearMade']\n",
        "df['AgeAtSale'] = df['saleyear'] - df['YearMade']\n",
        "df_valid['AgeAtSale'] = df_valid['saleyear'] - df_valid['YearMade']\n",
        "df.drop(['saledate', 'saleyear', 'YearMade'], axis=1, inplace=True)\n",
        "df_valid.drop(['saledate', 'saleyear', 'YearMade'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "kuWnmri9_Dyj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['TimesAppearing'] = df['MachineID'].map(df['MachineID'].value_counts())\n",
        "df.drop('MachineID', axis=1, inplace=True)\n",
        "df_valid['TimesAppearing'] = df_valid['MachineID'].map(df_valid['MachineID'].value_counts())\n",
        "df_valid.drop('MachineID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "_EjVXX0E_Liq"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['datasource', 'auctioneerID']] = df[['datasource', 'auctioneerID']].fillna(0.0)\n",
        "df_valid[['datasource', 'auctioneerID']] = df_valid[['datasource', 'auctioneerID']].fillna(0.0)"
      ],
      "metadata": {
        "id": "NgT_1dgS_OHw"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=['SalePrice']), df['SalePrice'], test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "zwghg1SZ_QDm"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statistics_df = compute_statistics(X_train)\n",
        "\n",
        "\"\"\" Use replace_outliers function in order to replace outliers\n",
        "in the 'MachineHoursCurrentMeter' column using statistics_df and IQR method \"\"\"\n",
        "X_train = replace_outliers(X_train, 'MachineHoursCurrentMeter', 'mean_without_extremes', statistics_df)\n",
        "X_test = replace_outliers(X_test, 'MachineHoursCurrentMeter', 'mean_without_extremes', statistics_df)\n",
        "df_valid = replace_outliers(df_valid, 'MachineHoursCurrentMeter', 'mean_without_extremes', statistics_df)\n",
        "\n",
        "\"\"\" Use replace_outliers function in order to replace outliers\n",
        "in the 'Age' columns using statistics_df and mean method \"\"\"\n",
        "X_train = replace_outliers(X_train, 'AgeAtSale', 'mean_without_extremes', statistics_df)\n",
        "X_test = replace_outliers(X_test, 'AgeAtSale', 'mean_without_extremes', statistics_df)\n",
        "df_valid = replace_outliers(df_valid, 'AgeAtSale', 'mean_without_extremes', statistics_df)\n",
        "\n",
        "\"\"\" Use replace_nans function in order to replace NaN values\n",
        "in the 'MachineHoursCurrentMeter' column using statistics_df and IQR method \"\"\"\n",
        "X_train = replace_nans(X_train, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "X_test = replace_nans(X_test, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "X_valid = replace_nans(df_valid, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "\n",
        "replace_nan_with_string(X_train)\n",
        "replace_nan_with_string(X_test)\n",
        "replace_nan_with_string(df_valid)"
      ],
      "metadata": {
        "id": "Y7uq-PHz_TLr"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed, X_test_transformed, X_valid_transformed=(\n",
        "    apply_one_hot_encoder(X_train, X_test, df_valid, X_train.select_dtypes(exclude=['number']).columns.tolist()))"
      ],
      "metadata": {
        "id": "pytydeDY_i8k"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_and_evaluate_model(X_train_transformed, X_test_transformed, y_train, y_test, [TREE_DEPTH],\n",
        "                                 LEVEL_OF_PARALLELISM, NUMBER_OF_TREES, MAX_FEATURES, MIN_SAMPLES_SPLIT,\n",
        "                                MIN_SAMPLES_LEAF)"
      ],
      "metadata": {
        "id": "cHkqk4Y__bVi",
        "outputId": "f362c610-3b00-4390-a37e-e0b0d50cd74a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Testing the model...\n",
            "Tree depth - 8\n",
            "STD Test - 22932.400534045228\n",
            "STD Train - 23081.526018913544\n",
            "RMSE Test - 11324.008170534435\n",
            "RMSE Train - 10856.934226244897\n",
            "Test/Train Ratio - 0.04124634469135091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "importance_list = [(importance, feature) for feature, importance in zip(feature_names, feature_importance)]\n",
        "importance_list.sort(reverse=True)\n",
        "print('-------FeatureImportance-------')\n",
        "for importance, feature in importance_list:\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "id": "ADdMo-ZNA1JM",
        "outputId": "34260cbb-15fe-4f16-cab8-bf71c0056ccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------FeatureImportance-------\n",
            "ProductSize: 0.28563368382457277\n",
            "AgeAtSale: 0.2203272488746134\n",
            "age: 0.09778244883449391\n",
            "Enclosure: 0.074361648129583\n",
            "ModelID: 0.0706229160308301\n",
            "uMounting_non_null_count: 0.06464128006253068\n",
            "fiModelDesc: 0.05340445439980177\n",
            "post_fi_Horsepower: 0.04914993664323257\n",
            "Hydraulics_Flow: 0.017210790148984842\n",
            "Enclosure_Type: 0.010940113048986202\n",
            "Blade_Type: 0.007606210983805998\n",
            "Engine_Horsepower: 0.007202839303828809\n",
            "post_fi_Metric Tons: 0.006991378166983932\n",
            "uTrack_non_null_count: 0.0052482551209697146\n",
            "SalesID: 0.005153438808763006\n",
            "ProductGroupDesc: 0.0038924675152081935\n",
            "ProductGroup: 0.003568123005345316\n",
            "uDrive_non_null_count: 0.0031550520692196393\n",
            "uDrive_any_non_null: 0.002895880914681679\n",
            "Category: 0.0023901601202956195\n",
            "uTrack_any_non_null: 0.0020211155225105617\n",
            "post_fi_Lb Operating Capacity: 0.0018583386601856277\n",
            "uBladeStick_any_non_null: 0.0012221138008103789\n",
            "uBladeStick_non_null_count: 0.0008382628651069526\n",
            "Track_Type: 0.0005763183466264921\n",
            "Hydraulics: 0.00033029124897777343\n",
            "MachineHoursCurrentMeter: 0.00020426262184956108\n",
            "datasource: 0.00016016599492855538\n",
            "uHydraulics_any_non_null: 0.00014844234441280731\n",
            "UsageBand: 0.00010943705337344645\n",
            "state: 7.127453123882395e-05\n",
            "post_fi_Ft Standard Digging Depth: 6.750708484812049e-05\n",
            "auctioneerID: 5.6504146426813626e-05\n",
            "Blade_Width: 5.4074555708893714e-05\n",
            "Steering_Controls: 4.930207471272727e-05\n",
            "Tire_Size: 3.7003190829771227e-05\n",
            "TimesAppearing: 1.5021430132267542e-05\n",
            "Undercarriage_Pad_Width: 2.23852058927726e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid_pred = model.predict(X_valid_transformed)\n",
        "# Create the prediction DataFrame with only 'SalesID' and 'Predicted_SalePrice'\n",
        "df_predictions = pd.DataFrame({\n",
        "    'SalesID': X_valid_transformed['remainder__SalesID'].astype(int),\n",
        "    'SalePrice': y_valid_pred\n",
        "})\n",
        "df_predictions.to_csv('valid_predictions_0.04.csv', index=False)"
      ],
      "metadata": {
        "id": "UM-nZnEoBCVo"
      },
      "execution_count": 103,
      "outputs": []
    }
  ]
}