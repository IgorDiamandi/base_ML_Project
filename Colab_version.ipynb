{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgP/cG1XDylEZhPMf4R3Db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorDiamandi/base_ML_Project/blob/BestResult/Colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMjKdkHdlpDo",
        "outputId": "bf33b32f-1666-44cf-b849-2552c627efc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'train.csv' already exists. Skipping download.\n",
            "File 'valid.csv' already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pathlib import Path\n",
        "\n",
        "def download_from_gdrive(url, filename):\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = url.split('/')[-2]\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download the file\n",
        "    if Path(filename).exists():\n",
        "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
        "    else:\n",
        "        gdown.download(download_url, filename, quiet=False)\n",
        "        print(f\"File downloaded as: {filename}\")\n",
        "\n",
        "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
        "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
        "# Example usage\n",
        "\n",
        "download_from_gdrive(train, 'train.csv')\n",
        "download_from_gdrive(valid, 'valid.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model functions\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_depth, level_of_parallelism, number_of_trees,\n",
        "                             max_features):\n",
        "    for depth in tree_depth:\n",
        "        model = RandomForestRegressor(\n",
        "                random_state=100,\n",
        "                n_jobs=level_of_parallelism,\n",
        "                n_estimators=number_of_trees,\n",
        "                max_depth=depth,\n",
        "                max_features=max_features)\n",
        "\n",
        "\n",
        "        print('Fitting the model...')\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        print('Testing the model...')\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        rmse_test = get_rmse(y_test, y_test_pred)\n",
        "        rmse_train = get_rmse(y_train, y_train_pred)\n",
        "\n",
        "        print(f'Tree depth - {depth}')\n",
        "        print(f'STD Test - {y_test.std()}')\n",
        "        print(f'STD Train - {y_train.std()}')\n",
        "        print(f'RMSE Test - {rmse_test}')\n",
        "        print(f'RMSE Train - {rmse_train}')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kOhM8hbAZ2Ko"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data functions\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Function to retrieve statistic parameters from numerical columns of the train dataframe\n",
        "def compute_statistics(df):\n",
        "    numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "    mean_values = numeric_df.mean()\n",
        "    iqr_values = numeric_df.quantile(0.75) - numeric_df.quantile(0.25)\n",
        "    zscore_values = (numeric_df.mean() / numeric_df.std()).mean()\n",
        "\n",
        "    # Mean without extremes (assuming extremes are values outside 1.5*IQR)\n",
        "    def mean_without_extremes(series):\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        filtered_series = series[(series >= Q1 - 1.5 * IQR) & (series <= Q3 + 1.5 * IQR)]\n",
        "        return filtered_series.mean()\n",
        "\n",
        "    mean_no_extremes_values = numeric_df.apply(mean_without_extremes)\n",
        "\n",
        "    statistics_df = pd.DataFrame({\n",
        "        'mean': mean_values,\n",
        "        'iqr': iqr_values,\n",
        "        'zscore': [zscore_values] * len(numeric_df.columns),\n",
        "        'mean_without_extremes': mean_no_extremes_values\n",
        "    })\n",
        "\n",
        "    statistics_df = statistics_df.T\n",
        "    return statistics_df\n",
        "\n",
        "\n",
        "def get_stat_value(method, column, statistics_df):\n",
        "  if method not in statistics_df.index:\n",
        "    raise ValueError(f\"Method {method} not found in statistics DataFrame\")\n",
        "  return statistics_df.loc[method, column]\n",
        "\n",
        "\n",
        "def replace_outliers(df, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "\n",
        "    # Define the threshold for identifying outliers\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the statistical method value\n",
        "    df[column] = df[column].apply(lambda x: stat_value if x < lower_bound or x > upper_bound else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def replace_nans(df, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "    df[column] = df[column].fillna(stat_value)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_rmse(y, y_pred):\n",
        "    return mean_squared_error(y, y_pred) ** 0.5\n",
        "\n",
        "\n",
        "def split_product_class_series(series):\n",
        "    equipment_type = []\n",
        "    details = []\n",
        "\n",
        "    for item in series:\n",
        "        if pd.isna(item):\n",
        "            equipment_type.append(None)\n",
        "            details.append(None)\n",
        "        else:\n",
        "            split_item = item.split(' - ', 1)\n",
        "            equipment_type.append(split_item[0])\n",
        "            details.append(split_item[1] if len(split_item) > 1 else None)\n",
        "\n",
        "    return pd.Series(equipment_type), pd.Series(details)"
      ],
      "metadata": {
        "id": "ju1gMoUpbq2b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df_valid = pd.read_csv('valid.csv')"
      ],
      "metadata": {
        "id": "KE2pPmGchFHY",
        "outputId": "4c67c5ed-ad45-4a77-e43e-567d5044ccd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6adbd7aebcbe>:1: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('train.csv')\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# convert 'YearMade' column to 'Age'\n",
        "\n",
        "df['Age'] = 2024 - df['YearMade']\n",
        "# Clamp 'YearMade' to a reasonable minimum before converting to datetime\n",
        "#df['AgeAtLastSale'] = pd.to_datetime(df['saledate']) - pd.to_datetime(df['YearMade'].clip(lower=1900), format='%Y')\n",
        "df.drop('YearMade', axis=1, inplace=True)\n",
        "\n",
        "df_valid['Age'] = 2024 - df_valid['YearMade']\n",
        "# Clamp 'YearMade' to a reasonable minimum before converting to datetime, and convert 'saledate' to datetime\n",
        "#df_valid['AgeAtLastSale'] = pd.to_datetime(df_valid['saledate']) - pd.to_datetime(df_valid['YearMade'].clip(lower=1900), format='%Y')\n",
        "df_valid.drop('YearMade', axis=1, inplace=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WguzR0M5JCN2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split 'fiProductDesc' column\n",
        "\n",
        "df['ProductClassName'],df['ProductClassCharacteristic'] = split_product_class_series(df['fiProductClassDesc'])\n",
        "df_valid['ProductClassName'],df_valid['ProductClassCharacteristic'] = split_product_class_series(df_valid['fiProductClassDesc'])"
      ],
      "metadata": {
        "id": "egV2666vdubL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert 'MachineID' column into 'TimesAppearing' column\n",
        "\n",
        "df['TimesAppearing'] = df['MachineID'].map(df['MachineID'].value_counts())\n",
        "df.drop('MachineID', axis=1, inplace=True)\n",
        "\n",
        "df_valid['TimesAppearing'] = df_valid['MachineID'].map(df_valid['MachineID'].value_counts())\n",
        "df_valid.drop('MachineID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5mGwN3CfX7o2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicated columns: 'ProductGroupDesc'\n",
        "\n",
        "df.drop(['ProductGroupDesc','ProductClassName'], axis=1, inplace=True)\n",
        "df_valid.drop(['ProductGroupDesc','ProductClassName'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "UmE4U4i3YbOR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace NaN values with 'missing' for non-numeric fields\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col].fillna('missing', inplace=True)\n",
        "\n",
        "\n",
        "for col in df_valid.columns:\n",
        "    if df_valid[col].dtype == 'object':\n",
        "        df_valid[col].fillna('missing', inplace=True)"
      ],
      "metadata": {
        "id": "1_pZv78LZK8t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wdjNvPLoFZ-Y"
      }
    },
    {
      "source": [
        "# replace NaN with -1 in 'ID' columns\n",
        "\n",
        "df[['datasource', 'auctioneerID']] = df[['datasource', 'auctioneerID']].fillna(-1)\n",
        "df_valid[['datasource', 'auctioneerID']] = df_valid[['datasource', 'auctioneerID']].fillna(-1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BvVsi9wVFQl5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split train and test data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=['SalePrice']), df['SalePrice'], test_size=0.3, random_state=100)"
      ],
      "metadata": {
        "id": "aAwx1KGiCa4D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get statistics dataframe using compute_statistics function from the X_train\n",
        "statistics_df = compute_statistics(X_train)"
      ],
      "metadata": {
        "id": "p0oIPYMgC3qG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Use replace_outliers function in order to replace outliers\n",
        "in the 'MachineHoursCurrentMeter' column using statistics_df and IQR method \"\"\"\n",
        "\n",
        "X_train = replace_outliers(X_train, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "X_test = replace_outliers(X_test, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "df_valid = replace_outliers(df_valid, 'MachineHoursCurrentMeter', 'iqr', statistics_df)"
      ],
      "metadata": {
        "id": "v9J6yjErhWlq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Use replace_outliers function in order to replace outliers\n",
        "in the 'Age' column using statistics_df and mean method \"\"\"\n",
        "\n",
        "X_train = replace_outliers(X_train, 'Age', 'mean', statistics_df)\n",
        "X_test = replace_outliers(X_test, 'Age', 'mean', statistics_df)\n",
        "df_valid = replace_outliers(df_valid, 'Age', 'mean', statistics_df)"
      ],
      "metadata": {
        "id": "q6y9pX3XJ9ED"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Use replace_nans function in order to replace outliers\n",
        "in the 'MachineHoursCurrentMeter' column using statistics_df and IQR method \"\"\"\n",
        "\n",
        "X_train = replace_nans(X_train, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "X_test = replace_nans(X_test, 'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "df_valid = replace_nans(df_valid, 'MachineHoursCurrentMeter', 'iqr', statistics_df)"
      ],
      "metadata": {
        "id": "CdAC3mv0LNxU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace NaN with 'Missing' in textual columns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == 'object':\n",
        "        if X_train[col].apply(type).nunique() > 1:\n",
        "            X_train[col] = X_train[col].fillna('Missing').astype(str)\n",
        "        X_train[col] = le.fit_transform(X_train[col])\n",
        "\n",
        "for col in X_test.columns:\n",
        "    if X_test[col].dtype == 'object':\n",
        "        if X_test[col].apply(type).nunique() > 1:\n",
        "            X_test[col] = X_test[col].fillna('Missing').astype(str)\n",
        "        X_test[col] = le.fit_transform(X_test[col])\n",
        "\n",
        "\n",
        "for col in df_valid.columns:\n",
        "    if df_valid[col].dtype == 'object':\n",
        "        if df_valid[col].apply(type).nunique() > 1:\n",
        "            df_valid[col] = df_valid[col].fillna('Missing').astype(str)\n",
        "        df_valid[col] = le.fit_transform(df_valid[col])\n",
        "\n"
      ],
      "metadata": {
        "id": "diV5Pq4UYqID"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical values to lables in all 3 dataframes\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Create the column transformer with OneHotEncoder\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fit the preprocessor on X_train and transform all dataframes\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "X_valid_transformed = preprocessor.transform(df_valid)\n",
        "\n",
        "# Convert transformed arrays back to dataframes with proper column names\n",
        "X_train_transformed = pd.DataFrame(X_train_transformed, columns=preprocessor.get_feature_names_out())\n",
        "X_test_transformed = pd.DataFrame(X_test_transformed, columns=preprocessor.get_feature_names_out())\n",
        "X_valid_transformed = pd.DataFrame(X_valid_transformed, columns=preprocessor.get_feature_names_out())"
      ],
      "metadata": {
        "id": "EkpKPP10PZpO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model = train_and_evaluate_model(X_train_transformed, X_test_transformed, y_train, y_test, [16],\n",
        "                                 -1, 20, 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4JPXcR2TBj9",
        "outputId": "f11955cb-48ec-44e9-e405-a1bc37194c5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Testing the model...\n",
            "Tree depth - 16\n",
            "STD Test - 22995.709788960918\n",
            "STD Train - 23054.44179044156\n",
            "RMSE Test - 17472.172461601975\n",
            "RMSE Train - 8030.635005842969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Model\n",
        "y_valid_pred = model.predict(X_valid_transformed)"
      ],
      "metadata": {
        "id": "79BH6lGfUCYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the prediction DataFrame with only 'SalesID' and 'Predicted_SalePrice'\n",
        "df_predictions = pd.DataFrame({\n",
        "    'SalesID': df_valid['SalesID'],\n",
        "    'SalePrice': y_valid_pred\n",
        "})\n",
        "df_predictions.to_csv('valid_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "NGk-DT4zjXkb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('valid_predictions.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TKkPGrIJpNCf",
        "outputId": "55bc5abd-799e-4862-9cac-0d21401c51c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5664e995-34c5-4a43-b24d-c2d44020bfbc\", \"valid_predictions.csv\", 307813)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print features importance order it descending\n",
        "\n",
        "feature_importances = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "importance_list = [(importance, feature) for feature, importance in zip(feature_names, feature_importances)]\n",
        "importance_list.sort(reverse=True)\n",
        "\n",
        "for importance, feature in importance_list:\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gstA97Hb3D_",
        "outputId": "485e7c22-962f-43e9-a7bd-a2cad3a35a63"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age: 0.23749700689228145\n",
            "ProductSize: 0.23449634626845442\n",
            "SalesID: 0.06690709687534345\n",
            "ModelID: 0.06571051972226238\n",
            "fiBaseModel: 0.06427993379022544\n",
            "fiModelDesc: 0.04090431034546565\n",
            "Enclosure: 0.037052397408903065\n",
            "fiProductClassDesc: 0.0348073903321477\n",
            "Coupler_System: 0.032273382920517055\n",
            "fiSecondaryDesc: 0.023885121099773084\n",
            "Tire_Size: 0.021912482296756505\n",
            "ProductClassCharacteristic: 0.020680655900391146\n",
            "fiModelDescriptor: 0.019679573570275603\n",
            "saledate: 0.014499584886193555\n",
            "state: 0.00978740484961463\n",
            "auctioneerID: 0.008860102517245979\n",
            "Blade_Extension: 0.006903125296805566\n",
            "MachineHoursCurrentMeter: 0.006075733068392157\n",
            "Blade_Width: 0.005240817976002786\n",
            "fiModelSeries: 0.004254969054711673\n",
            "ProductGroup: 0.003296742685730617\n",
            "Tip_Control: 0.0029293936487814923\n",
            "Ripper: 0.002852656880386089\n",
            "Engine_Horsepower: 0.0027516700766539704\n",
            "TimesAppearing: 0.002641738658928886\n",
            "datasource: 0.002531683923902871\n",
            "UsageBand: 0.0024313256946147626\n",
            "Travel_Controls: 0.0023893458149674644\n",
            "Ride_Control: 0.0023861328788374837\n",
            "Hydraulics_Flow: 0.0021597494678472343\n",
            "Hydraulics: 0.0017841419897311856\n",
            "Undercarriage_Pad_Width: 0.001676898294192578\n",
            "Blade_Type: 0.0016170229893502674\n",
            "Transmission: 0.00143496510351748\n",
            "Stick_Length: 0.0014234534066404803\n",
            "Drive_System: 0.0014070588201213928\n",
            "Pushblock: 0.0013107600530032869\n",
            "Grouser_Type: 0.0012042727076345741\n",
            "Scarifier: 0.0011411311327352367\n",
            "Coupler: 0.0010570669395599155\n",
            "Pattern_Changer: 0.0008473731114736611\n",
            "Pad_Type: 0.0006084548771515533\n",
            "Thumb: 0.0005764700530827693\n",
            "Enclosure_Type: 0.0004136853131430477\n",
            "Forks: 0.00041244245811314367\n",
            "Backhoe_Mounting: 0.0003392795847003594\n",
            "Steering_Controls: 0.00020034726288352303\n",
            "Track_Type: 0.00017221626069488448\n",
            "Differential_Type: 0.0001521077543101056\n",
            "Stick: 9.988677187399113e-05\n",
            "Turbocharged: 3.407153308820986e-05\n",
            "Grouser_Tracks: 8.498780584296564e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTKkQB14ia8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}