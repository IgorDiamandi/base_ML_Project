{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhUxlEtJXu8dFSaRijtwOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorDiamandi/base_ML_Project/blob/BestResult/Colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMjKdkHdlpDo",
        "outputId": "1a0dd6e8-1f86-414f-a69b-79f5f0bc1aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'train.csv' already exists. Skipping download.\n",
            "File 'valid.csv' already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from pathlib import Path\n",
        "\n",
        "def download_from_gdrive(url, filename):\n",
        "    # Extract the file ID from the URL\n",
        "    file_id = url.split('/')[-2]\n",
        "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    # Download the file\n",
        "    if Path(filename).exists():\n",
        "        print(f\"File '{filename}' already exists. Skipping download.\")\n",
        "    else:\n",
        "        gdown.download(download_url, filename, quiet=False)\n",
        "        print(f\"File downloaded as: {filename}\")\n",
        "\n",
        "train = 'https://drive.google.com/file/d/1guqSpDv1Q7ZZjSbXMYGbrTvGns0VCyU5/view?usp=drive_link'\n",
        "valid = 'https://drive.google.com/file/d/1j7x8xhMimKbvW62D-XeDfuRyj9ia636q/view?usp=drive_link'\n",
        "# Example usage\n",
        "\n",
        "download_from_gdrive(train, 'train.csv')\n",
        "download_from_gdrive(valid, 'valid.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, tree_depth, level_of_parallelism, number_of_trees,\n",
        "                             max_features, min_samples_split, min_samples_leaf):\n",
        "    for depth in tree_depth:\n",
        "        model = RandomForestRegressor(\n",
        "                random_state=100,\n",
        "                n_jobs=level_of_parallelism,\n",
        "                n_estimators=number_of_trees,\n",
        "                max_depth=depth,\n",
        "                max_features=max_features,\n",
        "                min_samples_split=min_samples_split,\n",
        "                min_samples_leaf=min_samples_leaf)\n",
        "\n",
        "\n",
        "        print('Fitting the model...')\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        print('Testing the model...')\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        rmse_test = get_rmse(y_test, y_test_pred)\n",
        "        rmse_train = get_rmse(y_train, y_train_pred)\n",
        "\n",
        "        print(f'Tree depth - {depth}')\n",
        "        print(f'STD Test - {y_test.std()}')\n",
        "        print(f'STD Train - {y_train.std()}')\n",
        "        print(f'RMSE Test - {rmse_test}')\n",
        "        print(f'RMSE Train - {rmse_train}')\n",
        "        print(f'Test/Train Ratio - {1-rmse_train/rmse_test}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kOhM8hbAZ2Ko"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMaFKj0o9URd",
        "outputId": "84252340-76b9-47eb-86c0-838738a1b2a4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn.preprocessing in /usr/local/lib/python3.10/dist-packages (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def compute_statistics(df):\n",
        "    numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "    mean_values = numeric_df.mean()\n",
        "    iqr_values = numeric_df.quantile(0.75) - numeric_df.quantile(0.25)\n",
        "    zscore_values = (numeric_df.mean() / numeric_df.std()).mean()\n",
        "\n",
        "    def mean_without_extremes(series):\n",
        "        Q1 = series.quantile(0.25)\n",
        "        Q3 = series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        filtered_series = series[(series >= Q1 - 1.5 * IQR) & (series <= Q3 + 1.5 * IQR)]\n",
        "        return filtered_series.mean()\n",
        "\n",
        "    mean_no_extremes_values = numeric_df.apply(mean_without_extremes)\n",
        "\n",
        "    statistics_df = pd.DataFrame({\n",
        "        'mean': mean_values,\n",
        "        'iqr': iqr_values,\n",
        "        'zscore': [zscore_values] * len(numeric_df.columns),\n",
        "        'mean_without_extremes': mean_no_extremes_values\n",
        "    })\n",
        "\n",
        "    return statistics_df.T\n",
        "\n",
        "\n",
        "def get_stat_value(method, column, statistics_df):\n",
        "    if method not in statistics_df.index:\n",
        "        raise ValueError(f\"Method {method} not found in statistics DataFrame\")\n",
        "    return statistics_df.loc[method, column]\n",
        "\n",
        "\n",
        "def replace_outliers(dfs, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "\n",
        "    def replace_outlier_values(df, column, lower_bound, upper_bound, stat_value):\n",
        "        df[column] = df[column].apply(lambda x: stat_value if x < lower_bound or x > upper_bound else x)\n",
        "        return df\n",
        "\n",
        "    updated_dfs = []\n",
        "    for df in dfs:\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        updated_df = replace_outlier_values(df.copy(), column, lower_bound, upper_bound, stat_value)\n",
        "        updated_dfs.append(updated_df)\n",
        "\n",
        "    return updated_dfs\n",
        "\n",
        "\n",
        "def replace_nans(dfs, column, method, statistics_df):\n",
        "    stat_value = get_stat_value(method, column, statistics_df)\n",
        "    for df in dfs:\n",
        "        df[column] = df[column].fillna(stat_value)\n",
        "    return dfs\n",
        "\n",
        "\n",
        "def get_rmse(y_log, y_pred_log):\n",
        "    y = np.expm1(y_log)\n",
        "    y_pred = np.expm1(y_pred_log)\n",
        "\n",
        "    return mean_squared_error(y, y_pred) ** 0.5\n",
        "\n",
        "'''\n",
        "def split_product_class_series(series):\n",
        "    equipment_type = []\n",
        "    details = []\n",
        "\n",
        "    for item in series:\n",
        "        if pd.isna(item):\n",
        "            equipment_type.append(None)\n",
        "            details.append(None)\n",
        "        else:\n",
        "            split_item = item.split(' - ', 1)\n",
        "            equipment_type.append(split_item[0])\n",
        "            details.append(split_item[1] if len(split_item) > 1 else None)\n",
        "\n",
        "    return pd.Series(equipment_type), pd.Series(details)\n",
        "'''\n",
        "\n",
        "\n",
        "def apply_one_hot_encoder(dfs, columns_to_encode, excluded_columns=[]):\n",
        "    one_hot_encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('onehot', one_hot_encoder, columns_to_encode)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "    # Fit and transform the first dataframe to get the feature names\n",
        "    X_encoded = [preprocessor.fit_transform(dfs[0])]\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    # Transform the remaining dataframes\n",
        "    for df in dfs[1:]:\n",
        "        X_encoded.append(preprocessor.transform(df))\n",
        "\n",
        "    # Create DataFrames from the encoded arrays\n",
        "    encoded_dfs = [pd.DataFrame(X.toarray(), columns=feature_names) for X in X_encoded]\n",
        "\n",
        "    # Remove excluded columns\n",
        "    for df in encoded_dfs:\n",
        "        df.drop(columns=excluded_columns, errors='ignore', inplace=True)\n",
        "\n",
        "    return encoded_dfs\n",
        "\n",
        "\n",
        "def unite_sparse_columns(df, columns_to_unite, new_column_name):\n",
        "    columns_to_unite = [col for col in columns_to_unite if col != new_column_name]\n",
        "\n",
        "    existing_columns = [col for col in columns_to_unite if col in df.columns]\n",
        "    missing_columns = [col for col in columns_to_unite if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: The following columns are missing and will be ignored: {missing_columns}\")\n",
        "\n",
        "    if not existing_columns:\n",
        "        raise ValueError(\"None of the specified columns to unite exist in the DataFrame.\")\n",
        "\n",
        "    df[new_column_name + '_non_null_count'] = df[existing_columns].notnull().sum(axis=1)\n",
        "    df[new_column_name + '_any_non_null'] = df[existing_columns].notnull().any(axis=1).astype(int)\n",
        "\n",
        "    if df[existing_columns].apply(lambda col: col.map(lambda x: isinstance(x, (int, float)))).all().all():\n",
        "        df[new_column_name + '_sum'] = df[existing_columns].sum(axis=1, skipna=True)\n",
        "\n",
        "    if df[existing_columns].apply(lambda col: col.map(lambda x: isinstance(x, str))).all().all():\n",
        "        df[new_column_name + '_mode'] = df[existing_columns].mode(axis=1)[0]\n",
        "\n",
        "    df = df.drop(columns=existing_columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def missing_values_imputation(df, target_column, feature_columns):\n",
        "    df_notnull = df.dropna(subset=[target_column])\n",
        "    df_null = df[df[target_column].isnull()]\n",
        "\n",
        "    if df_null.empty:\n",
        "        return df\n",
        "\n",
        "    X = df_notnull[feature_columns]\n",
        "    y = df_notnull[target_column]\n",
        "    X_null = df_null[feature_columns]\n",
        "\n",
        "    categorical_features = [col for col in feature_columns if df[col].dtype == 'object']\n",
        "    numerical_features = [col for col in feature_columns if df[col].dtype != 'object']\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', SimpleImputer(strategy='mean'), numerical_features),\n",
        "            ('cat', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "            ]), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', DecisionTreeClassifier(random_state=0, criterion='entropy'))\n",
        "    ])\n",
        "\n",
        "    model.fit(X, y)\n",
        "    df.loc[df[target_column].isnull(), target_column] = model.predict(X_null)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def split_fiProductClassDesc(df, column_name):\n",
        "    def parse_fiProductClassDesc(value):\n",
        "        match = re.match(r'^(.*) - (\\d+\\.\\d+) to (\\d+\\.\\d+) (.*)$', value)\n",
        "        if match:\n",
        "            category, low_value, high_value, characteristic = match.groups()\n",
        "            return category, float(low_value), float(high_value), characteristic\n",
        "        else:\n",
        "            return None, None, None, None\n",
        "\n",
        "    parsed_values = df[column_name].apply(parse_fiProductClassDesc)\n",
        "\n",
        "    df['Category'] = parsed_values.apply(lambda x: x[0])\n",
        "    df['LowValue'] = parsed_values.apply(lambda x: x[1])\n",
        "    df['HighValue'] = parsed_values.apply(lambda x: x[2])\n",
        "    df['Characteristic'] = parsed_values.apply(lambda x: x[3])\n",
        "\n",
        "    # Calculate the average of LowValue and HighValue\n",
        "    df['AverageValue'] = (df['LowValue'] + df['HighValue']) / 2\n",
        "\n",
        "    unique_characteristics = df['Characteristic'].dropna().unique()\n",
        "    for characteristic in unique_characteristics:\n",
        "        df[f'post_fi_{characteristic}'] = df.apply(\n",
        "            lambda row: row['AverageValue'] if row['Characteristic'] == characteristic else None,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    df = df.drop(columns=['LowValue', 'HighValue', 'Characteristic', 'AverageValue'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_lable_encoding(dfs, columns):\n",
        "    encoders = {col: LabelEncoder() for col in columns}\n",
        "\n",
        "    # Combine unique values for each column across all DataFrames\n",
        "    for col in columns:\n",
        "        unique_values = pd.concat([df[col] for df in dfs]).unique()\n",
        "        encoders[col].fit(unique_values)\n",
        "\n",
        "    # Encode the columns in all DataFrames\n",
        "    for df in dfs:\n",
        "        for col in columns:\n",
        "            df[col] = df[col].map(lambda s: '<unknown>' if s not in encoders[col].classes_ else s)\n",
        "\n",
        "    # Add '<unknown>' to the encoder classes and transform the columns\n",
        "    for col in columns:\n",
        "        encoders[col].classes_ = np.append(encoders[col].classes_, '<unknown>')\n",
        "\n",
        "    for df in dfs:\n",
        "        for col in columns:\n",
        "            df[col] = encoders[col].transform(df[col])\n",
        "\n",
        "    return dfs"
      ],
      "metadata": {
        "id": "ju1gMoUpbq2b"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Constants\n",
        "LEVEL_OF_PARALLELISM = -1\n",
        "NUMBER_OF_TREES = 100\n",
        "TREE_DEPTH = [17]\n",
        "MIN_SAMPLES_SPLIT = 3\n",
        "MIN_SAMPLES_LEAF = 2\n",
        "MAX_FEATURES = 0.8\n",
        "\n",
        "DONT_ENCODE_THEM_COLUMNS = ['ModelID', 'Income', 'AgeAtSale', 'SalesID', 'MachineHoursCurrentMeter']\n",
        "\n",
        "KILL_THEM_COLUMNS = ['uControl_any_non_null', 'uMounting_any_non_null', 'uHydraulics_non_null_count',\n",
        "                     'uControl_non_null_count', 'fiProductClassDesc', 'post_fi_Metric Tons',\n",
        "                     'post_fi_Ft Standard Digging Depth', 'post_fi_Lb Operating Capacity']\n",
        "\n",
        "SIZE_FIT_COLUMNS = ['fiModelDesc', 'post_fi_Horsepower',\n",
        "                    'Drive_System', 'Stick_Length', 'Undercarriage_Pad_Width',\n",
        "                    'post_fi_Metric Tons']\n",
        "\n",
        "USAGE_FIT_COLUMNS = ['AgeAtSale', 'MachineHoursCurrentMeter']\n",
        "\n",
        "DUPLICATE_COLUMNS = ['fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor']\n",
        "\n",
        "LABEL_ENCODING_COLUMNS = ['fiModelDesc', 'Enclosure', 'post_fi_Horsepower']\n",
        "\n",
        "COLUMN_GROUPS = {\n",
        "    'uBladeStick': ['Blade_Extension', 'Stick_Length', 'Stick'],\n",
        "    'uTrack': ['Pad_Type', 'Grouser_Type', 'Grouser_Tracks'],\n",
        "    'uMounting': ['Backhoe_Mounting', 'Forks', 'Pushblock', 'Ripper', 'Scarifier', 'Thumb'],\n",
        "    'uControl': ['Travel_Controls', 'Ride_Control', 'Transmission',\n",
        "                 'Pattern_Changer', 'Tip_Control', 'Coupler', 'Coupler_System'],\n",
        "    'uHydraulics': ['Turbocharged', 'Hydraulics', 'Hydraulics_Flow'],\n",
        "    'uDrive': ['Differential_Type', 'Drive_System']\n",
        "}"
      ],
      "metadata": {
        "id": "u1bWyRmf5Bsw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DTYPE_SPEC = {\n",
        "    13: 'str',\n",
        "    39: 'str',\n",
        "    40: 'str',\n",
        "    41: 'str'}\n",
        "\n",
        "df = pd.read_csv('train.csv', dtype=DTYPE_SPEC)\n",
        "df_valid = pd.read_csv('valid.csv', dtype=DTYPE_SPEC)"
      ],
      "metadata": {
        "id": "KE2pPmGchFHY"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#region Adding External Data\n",
        "# adding external states demographics data to both datasets\n",
        "#df = pd.merge(df, df_state_demographics, on='state', how='left')\n",
        "#df_valid = pd.merge(df_valid, df_state_demographics, on='state', how='left')\n",
        "#endregion"
      ],
      "metadata": {
        "id": "O0XbbsEl5LUU",
        "outputId": "02eef2fd-4560-4446-ec9e-8181153bcc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_state_demographics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-bf51b6a96893>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#region Adding External Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# adding external states demographics data to both datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_state_demographics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_state_demographics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#endregion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_state_demographics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#region Product size handling\n",
        "df = split_fiProductClassDesc(df, 'fiProductClassDesc')\n",
        "df_valid = split_fiProductClassDesc(df_valid, 'fiProductClassDesc')\n",
        "\n",
        "df = missing_values_imputation(df, 'ProductSize', SIZE_FIT_COLUMNS)\n",
        "df_valid = missing_values_imputation(df_valid, 'ProductSize', SIZE_FIT_COLUMNS)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "IyopRtNU5Nso"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Merging bad columns to one bad column\n",
        "# grouping columns with technical details\n",
        "for new_column, columns in COLUMN_GROUPS.items():\n",
        "    df = unite_sparse_columns(df, columns, new_column)\n",
        "    df_valid = unite_sparse_columns(df_valid, columns, new_column)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "r7j0vf7b5Psz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Convert dates to periods and categories\n",
        "df['saleyear'] = pd.to_datetime(df['saledate']).dt.year\n",
        "df_valid['saleyear'] = pd.to_datetime(df_valid['saledate']).dt.year\n",
        "\n",
        "df['salemonth'] = pd.to_datetime(df['saledate']).dt.month\n",
        "df_valid['salemonth'] = pd.to_datetime(df_valid['saledate']).dt.month\n",
        "\n",
        "df['saledayofweek'] = pd.to_datetime(df['saledate']).dt.dayofweek\n",
        "df_valid['saledayofweek'] = pd.to_datetime(df_valid['saledate']).dt.dayofweek\n",
        "\n",
        "df['AgeAtSale'] = df['saleyear'] - df['YearMade']\n",
        "df_valid['AgeAtSale'] = df_valid['saleyear'] - df_valid['YearMade']\n",
        "\n",
        "# drop unnecessary columns\n",
        "df.drop(['saledate', 'YearMade', 'saleyear'], axis=1, inplace=True)\n",
        "df_valid.drop(['saledate', 'YearMade', 'saleyear'], axis=1, inplace=True)\n",
        "#endregion\n"
      ],
      "metadata": {
        "id": "5gTvGnJ05RpY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Usage_band handling\n",
        "df = missing_values_imputation(df, 'UsageBand', USAGE_FIT_COLUMNS)\n",
        "df_valid = missing_values_imputation(df_valid, 'UsageBand', USAGE_FIT_COLUMNS)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "4D99iFqK5T29"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region convert 'MachineID' column into 'TimesAppearing' column\n",
        "df['TimesAppearing'] = df['MachineID'].map(df['MachineID'].value_counts())\n",
        "df.drop('MachineID', axis=1, inplace=True)\n",
        "df_valid['TimesAppearing'] = df_valid['MachineID'].map(df_valid['MachineID'].value_counts())\n",
        "df_valid.drop('MachineID', axis=1, inplace=True)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "b8lRI04a5Wrl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# region Replace NaN with 0 in 'ID' columns\n",
        "df[['datasource', 'auctioneerID']] = df[['datasource', 'auctioneerID']].fillna(0.0)\n",
        "df_valid[['datasource', 'auctioneerID']] = df_valid[['datasource', 'auctioneerID']].fillna(0.0)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "StVlx1Gf5e0k"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Dropping\n",
        "# drop unimportant columns\n",
        "df.drop(KILL_THEM_COLUMNS, axis=1, inplace=True)\n",
        "df_valid.drop(KILL_THEM_COLUMNS, axis=1, inplace=True)\n",
        "\n",
        "# drop duplicated columns\n",
        "df.drop(DUPLICATE_COLUMNS, axis=1, inplace=True)\n",
        "df_valid.drop(DUPLICATE_COLUMNS, axis=1, inplace=True)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "Rf99vLi55gk6"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Split train and test data + log normalization of the target column\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=['SalePrice']), df['SalePrice'], test_size=0.3, random_state=42)\n",
        "\n",
        "y_train = np.log1p(y_train)\n",
        "y_test = np.log1p(y_test)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "0HwQF1bR5inA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Get and use statistics dataframe\n",
        "statistics_df = compute_statistics(X_train)\n",
        "\n",
        "X_train, X_test, df_valid = replace_outliers([X_train, X_test, df_valid],\n",
        "                                             'MachineHoursCurrentMeter', 'iqr', statistics_df)\n",
        "\n",
        "X_train, X_test, df_valid = replace_outliers([X_train, X_test, df_valid],\n",
        "                                             'AgeAtSale', 'iqr', statistics_df)\n",
        "\n",
        "X_train, X_test, df_valid = replace_nans([X_train, X_test, df_valid],\n",
        "                                         'MachineHoursCurrentMeter',\n",
        "                                         'mean_without_extremes', statistics_df)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "W7UxwzvS5kRJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#region Applying encodings\n",
        "# label encoding\n",
        "X_train, X_test, df_valid = apply_lable_encoding([X_train, X_test, df_valid],\n",
        "                                                 LABEL_ENCODING_COLUMNS)\n",
        "\n",
        "# one-hot encoding\n",
        "X_train_transformed, X_test_transformed, X_valid_transformed = (\n",
        "    apply_one_hot_encoder([X_train, X_test, df_valid],\n",
        "                          X_train.select_dtypes(exclude=['number']).columns.tolist(),\n",
        "                          LABEL_ENCODING_COLUMNS))\n",
        "#endregion"
      ],
      "metadata": {
        "id": "BgkvRkdp5oB_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find nulls in X_train_transformed\n",
        "\n",
        "null_columns = X_train_transformed.columns[X_train_transformed.isnull().any()]\n",
        "print(null_columns)"
      ],
      "metadata": {
        "id": "Al0sGMou7JDZ",
        "outputId": "98d0a1fc-ca84-4cc4-8e0c-9c16b4c89ce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#region Train model\n",
        "model = train_and_evaluate_model(X_train_transformed, X_test_transformed, y_train, y_test, TREE_DEPTH,\n",
        "                                             LEVEL_OF_PARALLELISM, NUMBER_OF_TREES, MAX_FEATURES, MIN_SAMPLES_SPLIT,\n",
        "                                             MIN_SAMPLES_LEAF)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "djZzfy3a5pje",
        "outputId": "64dcef90-f10b-4d41-dbbd-9081df56d807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting the model...\n",
            "Testing the model...\n",
            "Tree depth - 17\n",
            "STD Test - 0.6936844473042342\n",
            "STD Train - 0.6935474937562746\n",
            "RMSE Test - 8473.899814198632\n",
            "RMSE Train - 7392.37850675058\n",
            "Test/Train Ratio - 0.12762970192730927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#region Feature importance\n",
        "#print(results_df)\n",
        "feature_importance = model.feature_importances_\n",
        "feature_names = X_train_transformed.columns\n",
        "importance_list = [(importance, feature) for feature, importance in zip(feature_names, feature_importance)]\n",
        "importance_list.sort(reverse=True)\n",
        "print('-------Feature Importance-------')\n",
        "for importance, feature in importance_list:\n",
        "    print(f\"{feature}: {importance}\")\n",
        "#endregion"
      ],
      "metadata": {
        "id": "5ERjbI5r5r4N",
        "outputId": "b7537f9d-30f6-4b8c-9825-2503dc05841a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------Feature Importance-------\n",
            "remainder__AgeAtSale: 0.18319647032666417\n",
            "onehot__ProductGroup_SSL: 0.10878678470402525\n",
            "onehot__ProductGroupDesc_Skid Steer Loaders: 0.10365088647965759\n",
            "remainder__ModelID: 0.09717687921482698\n",
            "remainder__post_fi_Horsepower: 0.07051023742534929\n",
            "onehot__ProductSize_Mini: 0.06841548228038705\n",
            "remainder__fiModelDesc: 0.06802027364864201\n",
            "onehot__ProductGroupDesc_Backhoe Loaders: 0.04809801079511014\n",
            "onehot__ProductGroup_BL: 0.0430840371886523\n",
            "remainder__SalesID: 0.03229840025946031\n",
            "onehot__ProductSize_Large / Medium: 0.026490216621974337\n",
            "onehot__ProductSize_Medium: 0.024688198364073856\n",
            "remainder__Enclosure: 0.019222466700292627\n",
            "onehot__ProductSize_Small: 0.012886399618552362\n",
            "onehot__Category_Skid Steer Loader: 0.011432717842182362\n",
            "onehot__ProductSize_Compact: 0.011232924773585345\n",
            "remainder__uMounting_non_null_count: 0.005877213216760793\n",
            "remainder__salemonth: 0.005872529664112986\n",
            "onehot__ProductSize_Large: 0.004279326474067164\n",
            "onehot__Blade_Type_Semi U: 0.004075955540891207\n",
            "onehot__UsageBand_Low: 0.0036167968766184166\n",
            "onehot__Tire_Size_nan: 0.0033844138120206254\n",
            "remainder__auctioneerID: 0.0033513370898304406\n",
            "remainder__MachineHoursCurrentMeter: 0.0028010704641925556\n",
            "remainder__saledayofweek: 0.0026657152284945223\n",
            "onehot__UsageBand_Medium: 0.0020603852969206526\n",
            "onehot__Track_Type_Rubber: 0.001873457061934059\n",
            "onehot__Category_None: 0.0017097492463364033\n",
            "remainder__datasource: 0.001687702229201107\n",
            "remainder__uDrive_any_non_null: 0.0014967923471699638\n",
            "onehot__UsageBand_High: 0.0013469239336384472\n",
            "remainder__TimesAppearing: 0.0013257837784753975\n",
            "onehot__Track_Type_Steel: 0.0012902993334206175\n",
            "onehot__Category_Hydraulic Excavator, Track: 0.0012086289295032372\n",
            "onehot__Category_Backhoe Loader: 0.0009575140308277095\n",
            "onehot__state_Texas: 0.0008122247397893274\n",
            "onehot__state_Florida: 0.0007630601374536138\n",
            "onehot__Category_Track Type Tractor, Dozer: 0.0006983867488700532\n",
            "onehot__Engine_Horsepower_nan: 0.0006915152274607123\n",
            "remainder__uDrive_non_null_count: 0.0006580292586416359\n",
            "onehot__state_California: 0.000637607005182469\n",
            "onehot__ProductGroupDesc_Motor Graders: 0.0005350980060565564\n",
            "onehot__Category_Wheel Loader: 0.0005245685175070153\n",
            "onehot__Enclosure_Type_nan: 0.0005173848315644724\n",
            "onehot__Blade_Width_nan: 0.00044678755927102584\n",
            "onehot__Blade_Type_PAT: 0.00044025628985189377\n",
            "onehot__ProductGroupDesc_Track Excavators: 0.00041742708999158764\n",
            "onehot__Blade_Type_nan: 0.0004139469638513612\n",
            "onehot__ProductGroupDesc_Track Type Tractors: 0.0003955924812683969\n",
            "onehot__ProductGroup_TEX: 0.00038513770331373323\n",
            "onehot__state_Washington: 0.0003765313316037416\n",
            "onehot__ProductGroup_MG: 0.0003727397854011544\n",
            "onehot__Steering_Controls_Conventional: 0.0003654453038322961\n",
            "onehot__Blade_Width_14': 0.0003564255630705905\n",
            "onehot__Undercarriage_Pad_Width_None or Unspecified: 0.00033566911436204883\n",
            "onehot__ProductGroup_TTT: 0.00032411531646956137\n",
            "onehot__Tire_Size_None or Unspecified: 0.000314650615501648\n",
            "onehot__state_Colorado: 0.0002715306120372835\n",
            "onehot__ProductGroup_WL: 0.0002679828488336763\n",
            "onehot__Enclosure_Type_None or Unspecified: 0.0002663126912528314\n",
            "onehot__state_New Jersey: 0.0002614462869183614\n",
            "onehot__state_Georgia: 0.00026089535171523883\n",
            "onehot__Category_Motorgrader: 0.0002600375435941652\n",
            "onehot__state_Maryland: 0.0002515365814468398\n",
            "onehot__state_Illinois: 0.0002464743988455209\n",
            "onehot__state_Ohio: 0.00024532066337126015\n",
            "remainder__uTrack_any_non_null: 0.00022661379706757182\n",
            "remainder__uTrack_non_null_count: 0.00021609733192865733\n",
            "onehot__ProductGroupDesc_Wheel Loader: 0.00021492106442848482\n",
            "onehot__Blade_Type_None or Unspecified: 0.000204413401737679\n",
            "onehot__state_North Carolina: 0.0002026356184080704\n",
            "onehot__state_Mississippi: 0.00020037689127477506\n",
            "onehot__Blade_Type_Straight: 0.00019940200783955816\n",
            "onehot__state_Pennsylvania: 0.0001964717403909809\n",
            "onehot__state_Minnesota: 0.00018760598070846537\n",
            "onehot__Enclosure_Type_Low Profile: 0.00018094467022162567\n",
            "onehot__state_South Carolina: 0.0001768340946317254\n",
            "onehot__Undercarriage_Pad_Width_nan: 0.00017455038727173108\n",
            "onehot__Tire_Size_20.5: 0.0001737972542604607\n",
            "onehot__state_Arizona: 0.00017059378077434955\n",
            "onehot__state_Connecticut: 0.0001667528901535994\n",
            "onehot__Blade_Type_U: 0.00016228421654912995\n",
            "onehot__Tire_Size_20.5\": 0.00016161553695106352\n",
            "onehot__state_Missouri: 0.00015581415990340667\n",
            "onehot__state_New York: 0.0001549938891406231\n",
            "onehot__state_Louisiana: 0.00015171226603393132\n",
            "remainder__uBladeStick_any_non_null: 0.00015144499615701526\n",
            "onehot__Steering_Controls_nan: 0.00014804686514057443\n",
            "onehot__state_Tennessee: 0.00014215198291549336\n",
            "onehot__state_Alabama: 0.00013441294279891092\n",
            "remainder__uBladeStick_non_null_count: 0.0001301713051604675\n",
            "onehot__state_Kentucky: 0.00011778964958858857\n",
            "onehot__state_Nevada: 0.00011587343448469057\n",
            "onehot__state_Utah: 9.286285981460148e-05\n",
            "onehot__state_Maine: 9.114451030496356e-05\n",
            "onehot__state_Arkansas: 8.685072490411568e-05\n",
            "onehot__Blade_Width_None or Unspecified: 8.577769400843628e-05\n",
            "onehot__state_Unspecified: 8.493058031982387e-05\n",
            "onehot__Engine_Horsepower_No: 8.026542319936754e-05\n",
            "onehot__Track_Type_nan: 7.992230470426128e-05\n",
            "onehot__state_Indiana: 7.942589500789399e-05\n",
            "onehot__Tire_Size_23.5: 7.767083203736563e-05\n",
            "onehot__Blade_Type_VPAT: 7.358378595277826e-05\n",
            "onehot__Tire_Size_14\": 6.541951501013159e-05\n",
            "onehot__state_Wisconsin: 6.310971688393743e-05\n",
            "onehot__state_New Mexico: 5.923006336334298e-05\n",
            "onehot__Tire_Size_26.5: 5.694794938241553e-05\n",
            "onehot__Undercarriage_Pad_Width_32 inch: 5.627608331022821e-05\n",
            "onehot__state_Idaho: 5.281836783486027e-05\n",
            "onehot__state_New Hampshire: 5.129932772890649e-05\n",
            "onehot__Tire_Size_17.5: 4.9421957576628614e-05\n",
            "onehot__state_Virginia: 4.7850966249919246e-05\n",
            "onehot__Blade_Width_12': 4.7212046614501905e-05\n",
            "onehot__Engine_Horsepower_Variable: 4.70134212505328e-05\n",
            "onehot__state_Wyoming: 4.598246304501618e-05\n",
            "onehot__state_Oregon: 4.4689461613707184e-05\n",
            "onehot__state_Michigan: 4.279694185018192e-05\n",
            "onehot__state_Montana: 4.1539842942163956e-05\n",
            "onehot__Undercarriage_Pad_Width_24 inch: 4.021149545711093e-05\n",
            "onehot__Undercarriage_Pad_Width_36 inch: 3.784096736080963e-05\n",
            "onehot__Undercarriage_Pad_Width_18 inch: 3.3555551032446146e-05\n",
            "onehot__state_Massachusetts: 3.219980490792633e-05\n",
            "onehot__Tire_Size_17.5\": 2.7827863613067272e-05\n",
            "onehot__Undercarriage_Pad_Width_28 inch: 2.7530737860985394e-05\n",
            "onehot__Undercarriage_Pad_Width_30 inch: 2.6873918944637275e-05\n",
            "onehot__Tire_Size_29.5: 2.646582191108629e-05\n",
            "onehot__Blade_Type_No: 2.523166117432363e-05\n",
            "onehot__state_Oklahoma: 2.2404648746040277e-05\n",
            "onehot__state_Iowa: 2.050080009621612e-05\n",
            "onehot__state_Nebraska: 1.9096549302819557e-05\n",
            "onehot__Blade_Type_Angle: 1.888981188233842e-05\n",
            "onehot__state_West Virginia: 1.8857664385036267e-05\n",
            "onehot__Undercarriage_Pad_Width_16 inch: 1.856709974705619e-05\n",
            "onehot__Undercarriage_Pad_Width_20 inch: 1.8336692268863858e-05\n",
            "onehot__state_Alaska: 1.8285501573865017e-05\n",
            "remainder__uHydraulics_any_non_null: 1.6478710586979364e-05\n",
            "onehot__Blade_Width_13': 1.6079390168362188e-05\n",
            "onehot__state_Kansas: 1.605985042141337e-05\n",
            "onehot__Enclosure_Type_High Profile: 1.4573284832309115e-05\n",
            "onehot__Tire_Size_13\": 1.3422029556969329e-05\n",
            "onehot__Blade_Width_16': 1.3160151511213758e-05\n",
            "onehot__Steering_Controls_Command Control: 7.84609173896708e-06\n",
            "onehot__state_Delaware: 7.21510148232983e-06\n",
            "onehot__Tire_Size_15.5: 7.137759948301808e-06\n",
            "onehot__Tire_Size_23.5\": 6.710725143255334e-06\n",
            "onehot__Undercarriage_Pad_Width_34 inch: 6.0900360252067996e-06\n",
            "onehot__state_North Dakota: 5.674706698778682e-06\n",
            "onehot__state_Hawaii: 5.164942553253791e-06\n",
            "onehot__Tire_Size_15.5\": 4.584434014641944e-06\n",
            "onehot__Blade_Type_Coal: 3.075558702851387e-06\n",
            "onehot__state_South Dakota: 2.754696095678076e-06\n",
            "onehot__Undercarriage_Pad_Width_27 inch: 2.158076725867319e-06\n",
            "onehot__Steering_Controls_Four Wheel Standard: 1.992064806882329e-06\n",
            "onehot__state_Vermont: 1.924543810652629e-06\n",
            "onehot__Undercarriage_Pad_Width_33 inch: 1.6663476248093652e-06\n",
            "onehot__Undercarriage_Pad_Width_31 inch: 1.1247035478295454e-06\n",
            "onehot__Undercarriage_Pad_Width_26 inch: 7.874272412921133e-07\n",
            "onehot__state_Puerto Rico: 7.339232007002749e-07\n",
            "onehot__Undercarriage_Pad_Width_22 inch: 6.738971527539597e-07\n",
            "onehot__Blade_Width_<12': 4.0903345028622114e-07\n",
            "onehot__Undercarriage_Pad_Width_14 inch: 3.2559782931794214e-07\n",
            "onehot__Tire_Size_7.0\": 3.1128842421711406e-07\n",
            "onehot__Tire_Size_10\": 2.6130760808458593e-07\n",
            "onehot__state_Rhode Island: 1.7669776327359611e-07\n",
            "onehot__Blade_Type_Landfill: 1.2056758438592695e-07\n",
            "onehot__Undercarriage_Pad_Width_15 inch: 1.018024803808297e-07\n",
            "onehot__Tire_Size_23.1\": 3.956169627887662e-08\n",
            "onehot__Steering_Controls_Wheel: 6.842293883519972e-09\n",
            "onehot__state_Washington DC: 0.0\n",
            "onehot__Undercarriage_Pad_Width_31.5 inch: 0.0\n",
            "onehot__Undercarriage_Pad_Width_25 inch: 0.0\n",
            "onehot__Tire_Size_10 inch: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#region Use Model\n",
        "y_valid_log_pred = model.predict(X_valid_transformed)\n",
        "y_valid_pred = np.expm1(y_valid_log_pred);\n",
        "\n",
        "# Create the prediction DataFrame with only 'SalesID' and 'Predicted_SalePrice'\n",
        "df_predictions = pd.DataFrame({\n",
        "    'SalesID': df_valid['SalesID'].astype(int),  # Ensure SalesID is included in df_valid\n",
        "    'SalePrice': y_valid_pred\n",
        "})\n",
        "\n",
        "df_predictions.to_csv('valid_predictions_0.2.csv', index=False)\n",
        "#endregion"
      ],
      "metadata": {
        "id": "YE2XNgqD5thW"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}